---
title: "Finals Code"
author: "Brian Jo Hsuan Lee | Jagjit Singh | Tanvir Khan"
date: '2022-05-10'
output: pdf_document
---

Load packages
```{r}
library(tidyverse)
library(caret)
library(earth)
library(rpart)
library(rpart.plot)
library(ranger)
library(purrr)
library(visdat)
library(h2o)
```

Clean data. Consolidate the historic team names into their corresponding current names; expand the weather detail column into boolean `dome`, `rain`, `fog`, and `snow` columns; update the spread values to those for the home team; rid seasons before 1979 due to incomplete betting line records; rid `schedule playoff` due to collinearity with the more informative `schedule week`; rid `spread_favored` and `weather details` as they are replaced by updated predictors; manually fill in the 2021 season SuperBowl final score as the dataset was created before said game.

```{r}
data = read_csv("spreadspoke_scores.csv", 
                    col_types = "iffffiiffddffiiic", 
                    col_select = c("schedule_season":"weather_detail")) %>%
  filter(
    schedule_season %in% (1979:2021)
  ) %>%
  mutate(
    schedule_season = droplevels(schedule_season),
    schedule_week = fct_collapse(schedule_week, 
                                 "SuperBowl" = c("Superbowl","SuperBowl"), 
                                 "WildCard" = c("Wildcard","WildCard")),
    schedule_week = fct_relevel(schedule_week, c(1:18, "WildCard", "Division", "Conference", "SuperBowl")),
    stadium = droplevels(stadium),
    score_home = ifelse(schedule_season == "2021" & schedule_week == "SuperBowl", 23, score_home),
    score_away = ifelse(schedule_season == "2021" & schedule_week == "SuperBowl", 20, score_away),
    dif = score_away - score_home, 
    weather_detail = replace(weather_detail, is.na(weather_detail), "Dry"),
    weather_detail = factor(weather_detail),
    team_home = fct_collapse(team_home, 
                             "Tennessee Titans" = c("Tennessee Titans", "Tennessee Oilers", "Houston Oilers"), 
                             "Washington Football Team" = c("Washington Football Team", "Washington Redskins"), 
                             "Las Vegas Raiders" = c("Oakland Raiders", "Los Angeles Raiders", "Las Vegas Raiders"), 
                             "Indianapolis Colts" = c("Baltimore Colts", "Indianapolis Colts"), 
                             "Los Angeles Chargers" = c("Los Angeles Chargers", "San Diego Chargers"), 
                             "Arizona Cardinals" = c("St. Louis Cardinals", "Phoenix Cardinals", "Arizona Cardinals"), 
                             "Los Angeles Rams" = c("Los Angeles Rams", "St. Louis Rams"), 
                             "New England Patriots" = c("New England Patriots", "Boston Patriots")),
    team_away = fct_collapse(team_away, 
                             "Tennessee Titans" = c("Tennessee Titans", "Tennessee Oilers", "Houston Oilers"), 
                             "Washington Football Team" = c("Washington Football Team", "Washington Redskins"), 
                             "Las Vegas Raiders" = c("Oakland Raiders", "Los Angeles Raiders", "Las Vegas Raiders"), 
                             "Indianapolis Colts" = c("Baltimore Colts", "Indianapolis Colts"), 
                             "Los Angeles Chargers" = c("Los Angeles Chargers", "San Diego Chargers"), 
                             "Arizona Cardinals" = c("St. Louis Cardinals", "Phoenix Cardinals", "Arizona Cardinals"), 
                             "Los Angeles Rams" = c("Los Angeles Rams", "St. Louis Rams"), 
                             "New England Patriots" = c("New England Patriots", "Boston Patriots")),
    team_away = fct_relevel(team_away, levels(team_home)),
    team_favorite_id = recode_factor(team_favorite_id,
                                     "MIA" = "Miami Dolphins",
                                     "TEN" = "Tennessee Titans",
                                     "LAC" = "Los Angeles Chargers",
                                     "GB" = "Green Bay Packers",
                                     "ATL" = "Atlanta Falcons",
                                     "BUF" = "Buffalo Bills",
                                     "DET" = "Detroit Lions",
                                     "PIT" = "Pittsburgh Steelers",
                                     "SF" = "San Francisco 49ers",
                                     "ARI" = "Arizona Cardinals",
                                     "WAS" = "Washington Football Team",
                                     "LAR" = "Los Angeles Rams",
                                     "CLE" = "Cleveland Browns",
                                     "DAL" = "Dallas Cowboys",
                                     "DEN" = "Denver Broncos",
                                     "MIN" = "Minnesota Vikings",
                                     "NYJ" = "New York Jets",
                                     "LVR" = "Las Vegas Raiders",
                                     "PHI" = "Philadelphia Eagles",
                                     "IND" = "Indianapolis Colts",
                                     "NE" = "New England Patriots",
                                     "KC" = "Kansas City Chiefs",
                                     "NYG" = "New York Giants",
                                     "CHI" = "Chicago Bears",
                                     "NO"= "New Orleans Saints",
                                     "CIN" = "Cincinnati Bengals",
                                     "SEA" = "Seattle Seahawks",
                                     "TB" = "Tampa Bay Buccaneers",
                                     "JAX" = "Jacksonville Jaguars",
                                     "CAR" = "Carolina Panthers",
                                     "BAL" = "Baltimore Ravens",
                                     "HOU" = "Houston Texans",
                                     .default = "None"),
    spread_home = ifelse(as.character(team_away) == as.character(team_favorite_id), abs(spread_favorite), spread_favorite),
    dome = ifelse(((as.character(weather_detail) == "DOME") | (as.character(weather_detail) == "DOME (Open Roof)")), TRUE, FALSE),
    fog = ifelse((as.character(weather_detail) == "Fog") | (as.character(weather_detail) == "Rain | Fog") | (as.character(weather_detail) == "Snow | Fog"), T, F),
    rain = ifelse((as.character(weather_detail) == "Rain") | (as.character(weather_detail) == "Rain | Fog") | (as.character(weather_detail) == "Snow | Freezing Rain"), T, F),
    snow = ifelse((as.character(weather_detail) == "Snow") | (as.character(weather_detail) == "Snow | Fog"), T, F),
  ) %>% 
  select(-score_home, -score_away, -team_favorite_id, -spread_favorite, -weather_detail)
```

```{r}
vis_miss(data)
```

Transform 
Partition data into training and testing sets, and define the resampling method.

```{r}
set.seed(2022)

# partition data into training and testing sets into randomized 4:1 splits
train_index = createDataPartition(y = data$dif, p = 0.8, list = FALSE)
train_pred = 
  data %>% 
  filter(row_number() %in% train_index) %>% 
  select(-dif)
test_pred = 
  data %>% 
  filter(!row_number() %in% train_index) %>% 
  select(-dif)
```

In the midterm, we replaced the NA values in weather temperature, humidity and wind speed with each of their grand averages. However, missing data imputation could be improved. Transformation on continuous predictors would also help improve predictions by scaling and standardizing their weights on the response. Here, we use the Yeo-Johnson transformation for non-positive numeric predictors and bag imputation to fill in the missing weather data. 
```{r}
vis_miss(train_pred)
trans_imp = preProcess(train_pred, method = c("YeoJohnson", "zv", "bagImpute"))
ti_train_pred = predict(trans_imp, train_pred)
vis_miss(ti_train_pred)
ti_test_pred = predict(trans_imp, test_pred)
```

Re-generate our training and testing data for analysis. Reserve an unpartitioned dataset for exploratory analysis.
```{r}
eda_data = 
  bind_rows(
    data %>%
      filter(row_number() %in% train_index) %>% 
      select(dif) %>%
      bind_cols(., ti_train_pred),
    data %>%
      filter(!row_number() %in% train_index) %>% 
      select(dif) %>%
      bind_cols(., ti_test_pred))
train_data = 
  data %>% 
  filter(row_number() %in% train_index) %>% 
  select(dif) %>% 
  bind_cols(., ti_train_pred) %>% 
  select(-schedule_season, -schedule_playoff)
test_data = 
  data %>% 
  filter(!row_number() %in% train_index) %>% 
  select(dif) %>% 
  bind_cols(., ti_test_pred) %>% 
  select(-schedule_season, -schedule_playoff)
train_cont_data = 
  train_data %>% 
  select(dif, over_under_line, spread_home, weather_temperature, weather_wind_mph, weather_humidity)
```

Some model building methods may require matrices with indicator varibles for categorical predictors. 
```{r}
# create matrices of predictors 
train_pred = model.matrix(dif ~ ., train_data)[ ,-1]
train_cont_pred = model.matrix(dif ~ ., train_cont_data)[ ,-1]
test_pred = model.matrix(dif ~ ., test_data)[ ,-1]

# vectors of response
train_resp = train_data$dif
test_resp = test_data$dif

ctrl = trainControl(method = "cv")
```

Elastic Net is our preferred model from the midterm project. We will compare our new models with this. 
```{r}
set.seed(2022)
# Fit an elastic net model (L1 & L2 regularization, alpha = [0,1])
enet_fit = train(dif ~ .,
                 data = train_data,
                 method = "glmnet",
                 tuneGrid = expand.grid(alpha = seq(0, 1, length = 5),
                                        lambda = exp(seq(1, -3, length = 100))),
                 trControl = ctrl)
ggplot(enet_fit, highlight = TRUE)
```










**DECISION TREE** - Using CARET
```{r}
set.seed(2022)
rpart.fit = train(dif ~ ., 
                  data = train_data,
                  method = "rpart",
                  tuneGrid = data.frame(cp = exp(seq(-50,-5, length = 25))),
                  trControl = ctrl)

ggplot(rpart.fit, highlight = TRUE)

rpart.plot(rpart.fit$finalModel)

rpart.fit$bestTune
```

Train Error
```{r}
rf_prediction_train = predict(rpart.fit, newdata = train_data)
rmse_rf_train = RMSE(rf_prediction_train, train_resp); rmse_rf_train
```

Test Error
```{r}
rf_prediction = predict(rpart.fit, newdata = test_data)
rmse_rf <- RMSE(rf_prediction, test_resp); rmse_rf
```










**CONDITIONAL INFERENCE TREE** - Using CARET
```{r}
set.seed(2022)
ctree.fit <- train(dif ~ . ,
                   train_data,
                   method = "ctree",
                   tuneGrid = data.frame(mincriterion = 1-exp(seq(-50, -5, length = 25))),
                   trControl = ctrl)


ggplot(ctree.fit, highlight = TRUE)
```

```{r}
ctree.fit$best
```

```{r}
plot(ctree.fit$finalModel)
```









**Random Forest** - Using Caret
```{r}
rf.grid = expand.grid(mtry = c(1:5),
                      splitrule = "variance",
                      min.node.size = 1:6)

set.seed(2022)
rf.fit <- train(dif ~ . ,
                train_data,
                method = "ranger",tuneGrid = rf.grid,
                trControl = ctrl)

ggplot(rf.fit, highlight = TRUE)
```

```{r}
rf.fit$bestTune
```








**GBM** - Using CARET 
```{r}
gbm.grid <- expand.grid(n.trees = c(1,50),
                        interaction.depth = 1:5,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = 1)
set.seed(2022)
gbm.fit <- train(dif ~ . , 
                 train_data,
                 method = "gbm",
                 tuneGrid = gbm.grid,
                 trControl = ctrl,
                 verbose = FALSE)

ggplot(gbm.fit, highlight = TRUE)
```


```{r}
ggplot(gbm.fit, highlight = TRUE)
```



```{r}
gbm.fit$bestTune
```







The super learner is an ensemble method using a variety of models as its base models. I want to build one using RF and GBM, so will need the optimal tuning parameters. 
```{r}
h2o.init()
```

2 model ensemble
```{r}
# Data preparation
train_h2o = as.h2o(train_data)
test_h2o = as.h2o(test_data)
y <- "dif"
x <- setdiff(names(train_h2o), y)

# Train & cross-validate a GBM:
my_gbm <- h2o.gbm(x = x,
                  y = y,
                  training_frame = train_h2o,
                  distribution = "gaussian",
                  ntrees = 10,
                  max_depth = 3,
                  min_rows = 2,
                  learn_rate = 0.2,
                  nfolds = nfolds,
                  keep_cross_validation_predictions = TRUE,
                  seed = 1)

# Train & cross-validate a RF:
my_rf <- h2o.randomForest(x = x,
                          y = y,
                          training_frame = train_h2o,
                          ntrees = 50,
                          nfolds = nfolds,
                          keep_cross_validation_predictions = TRUE,
                          seed = 1)

# Train a stacked ensemble using the GBM and RF above:
ensemble <- h2o.stackedEnsemble(x = x,
                                y = y,
                                training_frame = train_h2o,
                                base_models = list(my_gbm, my_rf))

# Evaluate ensemble performance on test set
perf <- h2o.performance(ensemble, newdata = test_h2o)
ensemble_auc_test <- h2o.auc(perf)

# Compare the ensemble to GBM and RF (baseline learners) performance on the test set
perf_gbm_test <- h2o.performance(my_gbm, newdata = test_h2o)
perf_rf_test <- h2o.performance(my_rf, newdata = test_h2o)
baselearner_best_auc_test <- max(h2o.auc(perf_gbm_test), 
                                 h2o.auc(perf_rf_test))

print(sprintf("Best Base-learner Test AUC: %s", baselearner_best_auc_test))
print(sprintf("Ensemble Test AUC: %s", ensemble_auc_test))

# Dif prediction on test set
pred <- h2o.predict(ensemble, newdata = test)
```

Grid of Models
```{r}
# GBM hyperparamters
learn_rate_opt <- c(0.01, 0.03)
max_depth_opt <- c(3, 4, 5, 6, 9)
sample_rate_opt <- c(0.7, 0.8, 0.9, 1.0)
col_sample_rate_opt <- c(0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8)
hyper_params <- list(learn_rate = learn_rate_opt,
                     max_depth = max_depth_opt,
                     sample_rate = sample_rate_opt,
                     col_sample_rate = col_sample_rate_opt)

search_criteria <- list(strategy = "RandomDiscrete",
                        max_models = 3,
                        seed = 1)

gbm_grid <- h2o.grid(algorithm = "gbm",
                     grid_id = "gbm_grid_binomial",
                     x = x,
                     y = y,
                     training_frame = train,
                     ntrees = 10,
                     seed = 1,
                     nfolds = nfolds,
                     keep_cross_validation_predictions = TRUE,
                     hyper_params = hyper_params,
                     search_criteria = search_criteria)

# Train a stacked ensemble using the GBM grid
ensemble <- h2o.stackedEnsemble(x = x,
                                y = y,
                                training_frame = train,
                                base_models = gbm_grid@model_ids)

# Eval ensemble performance on a test set
perf <- h2o.performance(ensemble, newdata = test)

# Compare to base learner performance on the test set
.getauc <- function(mm) h2o.auc(h2o.performance(h2o.getModel(mm), newdata = test))
baselearner_aucs <- sapply(gbm_grid@model_ids, .getauc)
baselearner_best_auc_test <- max(baselearner_aucs)
ensemble_auc_test <- h2o.auc(perf)
print(sprintf("Best Base-learner Test AUC:  %s", baselearner_best_auc_test))
print(sprintf("Ensemble Test AUC:  %s", ensemble_auc_test))

# Generate predictions on a test set (if neccessary)
pred <- h2o.predict(ensemble, newdata = test)
```

```{r}
h2o.shutdown()
```
